# 思考

Redis主从架构，slave节点宕机恢复后可以找master节点同步数据，那么master节点宕机怎么办？

主从集群下，无法自动进行主从切换，master节点挂掉，该主从集群无法执行写操作，只能执行读操作。

引出哨兵：

使用哨兵Sentinel监控master是否故障，如果故障了根据投票数选举自动将某一个从库转换成新主库，继续对外服务。

# Redis哨兵

Redis提供了哨兵（Sentinel）机制来实现主从集群的自动故障恢复。

## 作用

- 监控：Sentinel 会不断检查您的master和slave是否正常工作，监控Redis的运行状态。

- 自动故障转移：当master宕机后，实现故障转移，自动将slave切换成新的master。当故障实例回复后会切换成slave以新的master为主。

- 消息通知：当集群发生故障转移时，Sentinel会将故障转移的结果噶送给所有连接到它的Redis客户端。

- 配置中心：提供和维护集群的配置信息，如节点地址、角色等。确保客户端能够正确地连接到Redis节点

# 搭建哨兵集群

![image-20240306121019675](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403061210935.png)

这里我们搭建一个三个节点形成的Sentinel集群，来监管之前的Redis主从集群。

三个sentinel实现自动监控和维护集群，不存放数据

备注：生产中，哨兵部署在不同机房不同服务器上，而且哨兵只负责进行监管Redis主从集群，三个哨兵全挂掉的情况几乎不会出现。

在Redis Sentinel架构中，多个Sentinel实例组成一个Sentinel集群，共同负责监控和管理Redis主从集群。这些Sentinel实例之间是对等关系，它们通过相互通信来协作完成监控和故障转移的任务。

实例信息如下：

以192.168.122.132实例克隆出三个新的实例，主从架构还是使用之前配置的一主二从。

| 节点     | IP              | port  |
| -------- | --------------- | ----- |
| sentine1 | 192.168.122.135 | 26379 |
| sentine2 | 192.168.122.136 | 26380 |
| sentine3 | 192.168.122.137 | 26381 |
| master   | 192.168.122.132 | 7001  |
| slave    | 192.168.122.133 | 7002  |
| slave    | 192.168.122.134 | 7003  |



这里只以sentine1为例

```shell
[root@sentinel1 ~]# cd /opt/redis-6.2.6
[root@sentinel1 redis-6.2.6]# ll
总用量 428
-rw-rw-r--.  1 root root 33624 10月  4 2021 00-RELEASENOTES
-rw-rw-r--.  1 root root    51 10月  4 2021 BUGS
-rw-rw-r--.  1 root root  5026 10月  4 2021 CONDUCT
-rw-rw-r--.  1 root root  3384 10月  4 2021 CONTRIBUTING
-rw-rw-r--.  1 root root  1487 10月  4 2021 COPYING
drwxrwxr-x.  7 root root   213 3月   5 15:23 deps
-rw-rw-r--.  1 root root    11 10月  4 2021 INSTALL
-rw-rw-r--.  1 root root   151 10月  4 2021 Makefile
-rw-rw-r--.  1 root root  6888 10月  4 2021 MANIFESTO
-rw-rw-r--.  1 root root 21567 10月  4 2021 README.md
-rw-r--r--.  1 root root 93795 3月   5 16:56 redis7001.conf
-rw-rw-r--.  1 root root 93791 3月   5 15:38 redis.conf
-rw-r--r--.  1 root root 93724 3月   5 15:26 redis.conf.bak
-rwxrwxr-x.  1 root root   275 10月  4 2021 runtest
-rwxrwxr-x.  1 root root   279 10月  4 2021 runtest-cluster
-rwxrwxr-x.  1 root root  1079 10月  4 2021 runtest-moduleapi
-rwxrwxr-x.  1 root root   281 10月  4 2021 runtest-sentinel
-rw-rw-r--.  1 root root 13768 10月  4 2021 sentinel.conf
drwxrwxr-x.  3 root root 12288 3月   5 15:24 src
drwxrwxr-x. 11 root root   182 10月  4 2021 tests
-rw-rw-r--.  1 root root  3055 10月  4 2021 TLS.md
drwxrwxr-x.  9 root root  4096 10月  4 2021 utils
```

## sentinel.conf重要参数说明

- bind：服务监听地址，用于客户端连接，默认本机地址
- daemonize：是否以后台方式运行。
- protected-mode：是否开启安全保护模式
- port:端口号
- logfile:日志文件路径
- pidfile: pid文件路径

- dir: 工作目录

- sentinel monitor <master-name> <ip> <redis-port> <quorum>:设置要监控的master服务器，quorum表示确认客观下线的最少的哨兵数量，统一故障迁移的法定票数。

我们知道，网络是不可靠的，有时候一个sentinel会因为网络堵塞而误以为一个master redis已经死掉了，在sentinel集群环境下需要多个sentinel互相沟通来确认某个master是否真的死了，quorum这个参数是进行客观下线的一个依据，意思是至少有quorum个sentinel认为这个master有故障，才会对这个master进行下线以及故障转移。因为有的时候，某个sentinel节点可能因为自身网络原因，导致无法连接master，而此时master并没有出现故障，所以，这就需要多个sentinel都一致认为该master有问题，才可以进行下一步操作，这就保证T了公平性和高可用。

- sentinel auth-pass <master-name> <password>：master设置了密码，连接master服务的密码。

其他参数：

- sentinel down-after-milliseconds <master-name> <milliseconds>：判断一个主节点（master）是否下线的超时时间。指定多少毫秒之后，Sentinel 仍然无法从主节点那里获得响应，此时哨兵主观认为主节点下线。
- sentinel parallel-syncs <master-name> <nums>：指定在执行故障转移（failover）时，可以有多少个从服务器（slave）并行地从新的主服务器（new master）同步数据。
- sentinel failover-timeout <master-name> <milliseconds>：设置故障转移（failover）操作的超时时间，进行故障转移时，如果超过设置的毫秒，表示故障转移失败。
- sentinel notification-script <master-name> <script-path>：允许你指定一个脚本，当 Sentinel 执行某些关键操作时，比如开始或结束故障转移（failover）时，该脚本会被调用。
- sentinel client-reconfig-script <master-name> <script-path>：允许你指定一个脚本，当 Sentinel 执行故障转移后，该脚本会被调用，以便重新配置连接到故障转移前的主服务器的客户端。



## sentinel.conf配置

创建sentinel26379.conf文件

```shell
[root@sentinel1 redis-6.2.6]# vim sentinel26379.conf
```

1.bind 0.0.0.0 #服务监听地址，监听所有地址

2.daemonize yes  # 开启守护进程模式，Redis在后台运行，如果想在前台查看日志，可以关闭

4.port 26379 #端口26379

5.logfile "/myredis/sentinel26379.log"  #日志文件地址

6.pidfile "/var/run/redis-sentinel26379.pid" #PID写入的文件目录

7.dir /myredis  #指定当前工作目录

8.sentinel monitor mymaster 192.168.122.132 7001 2 #监控主机192.168.122.162:7001，哨兵可以同时监控多个master，一行一个。

9.sentinel auth-pass mymaster root   #连接master服务的密码

输入一下内容：

```
bind 0.0.0.0
daemonize yes
port 26379
logfile "/myredis/sentinel26379.log"
pidfile "/var/run/redis-sentinel26379.pid"
dir /myredis
sentinel monitor mymaster 192.168.122.132 7001 2
sentinel auth-pass mymaster root
```

## 配置master7001的redis7001.conf

```
masterauth root
```

7001后续可能会变成从机，连接新主机需要设置masterauth root，不然可能报错master_link_status：down。

# 测试

## 启动redis主从

1.启动一主二从

```shell
[root@redis-7001 ~]# redis-server /opt/redis-6.2.6/redis7001.conf 
[root@redis-7001 ~]# redis-cli -a root -p 7001
Warning: Using a password with '-a' or '-u' option on the command line interface may not be safe.
127.0.0.1:7001> 
```

```shell
[root@redis-7002 ~]# redis-server /opt/redis-6.2.6/redis7002.conf 
[root@redis-7002 ~]# redis-cli -a root -p 7002
Warning: Using a password with '-a' or '-u' option on the command line interface may not be safe.
127.0.0.1:7002> 
```

```shell
[root@redis-7003 ~]# redis-server /opt/redis-6.2.6/redis7003.conf 
[root@redis-7003 ~]# redis-cli -a root -p 7003
Warning: Using a password with '-a' or '-u' option on the command line interface may not be safe.
127.0.0.1:7003>
```

2.查看master7001的主从关系

两台从机已经连接成功

```shell
127.0.0.1:7001> info replication
# Replication
role:master
connected_slaves:2
slave0:ip=192.168.122.133,port=7002,state=online,offset=280,lag=1
slave1:ip=192.168.122.134,port=7003,state=online,offset=280,lag=1
master_failover_state:no-failover
master_replid:931607db8c3d02fe95ccc3ed57119300fd24139f
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:280
second_repl_offset:-1
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:1
repl_backlog_histlen:280
```

3.测试同步功能

master写入k7

```shell
127.0.0.1:7001> set k7 v7
OK
127.0.0.1:7001> get k7
"v7"
```

slave读取k7

```shell
127.0.0.1:7002> get k7
"v7"
```

```shell
127.0.0.1:7003> get k7
"v7"
```

## 启动哨兵集群

1.启动三台哨兵

26379

```shell
[root@sentinel1 redis-6.2.6]# redis-sentinel sentinel26379.conf --sentinel
```

26380

```shell
[root@sentinel2 redis-6.2.6]# redis-sentinel sentinel26380.conf --sentinel
```

26381

```shell
[root@sentinel3 redis-6.2.6]# redis-sentinel sentinel26381.conf --sentinel
```



2.查看sentinel日志，正常启动

```shell
1272:X 06 Mar 2024 15:14:16.381 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
1272:X 06 Mar 2024 15:14:16.381 # Redis version=6.2.6, bits=64, commit=00000000, modified=0, pid=1272, just started
1272:X 06 Mar 2024 15:14:16.381 # Configuration loaded
1272:X 06 Mar 2024 15:14:16.382 * Increased maximum number of open files to 10032 (it was originally set to 1024).
1272:X 06 Mar 2024 15:14:16.382 * monotonic clock: POSIX clock_gettime
1272:X 06 Mar 2024 15:14:16.385 * Running mode=sentinel, port=26379.
1272:X 06 Mar 2024 15:14:16.385 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.
1272:X 06 Mar 2024 15:14:16.387 # Sentinel ID is c710c5461f8d6acec712d350cd8a34a5fef34d9f
1272:X 06 Mar 2024 15:14:16.387 # +monitor master mymaster 192.168.122.132 7001 quorum 2
1272:X 06 Mar 2024 15:14:16.390 * +slave slave 192.168.122.133:7002 192.168.122.133 7002 @ mymaster 192.168.122.132 7001
1272:X 06 Mar 2024 15:14:16.392 * +slave slave 192.168.122.134:7003 192.168.122.134 7003 @ mymaster 192.168.122.132 7001
1272:X 06 Mar 2024 15:15:27.755 * +sentinel sentinel d5511f23142adb41942b0f22dffea05415dc78c7 192.168.122.136 26380 @ mymaster 192.168.122.132 7001
1272:X 06 Mar 2024 15:16:07.628 * +sentinel sentinel 5e1b76aa73f25d91b0db2a4870073393ae9484e7 192.168.122.137 26381 @ mymaster 192.168.122.132 7001
```

sentinel  c710c5461f8d6acec712d350cd8a34a5fef34d9f 192.168.122.135 26379

sentinel d5511f23142adb41942b0f22dffea05415dc78c7 192.168.122.136 26380

sentinel 5e1b76aa73f25d91b0db2a4870073393ae9484e7 192.168.122.137 26381

三台sentinel 监控主机7001

## 再次测试主从同步

依然正常同步

```shell
127.0.0.1:7001> info replication
# Replication
role:master
connected_slaves:2
slave0:ip=192.168.122.133,port=7002,state=online,offset=66011,lag=0
slave1:ip=192.168.122.134,port=7003,state=online,offset=65866,lag=0
master_failover_state:no-failover
master_replid:931607db8c3d02fe95ccc3ed57119300fd24139f
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:66011
second_repl_offset:-1
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:1
repl_backlog_histlen:66011
127.0.0.1:7001> set k8 v8
OK
127.0.0.1:7001> get k8
"v8"
```

```shell
127.0.0.1:7002> get k8
"v8"
```

```shell
127.0.0.1:7003> get k8
"v8"
```

## 查看sentinel的配置

sentinel26379.conf发现除了之前我们配置的参数外，新增了Generated by CONFIG REWRITE以下配置

哨兵就知道要监控哪个redis节点，有哪些从节点，其他哨兵节点有哪些，另外两个sentinel配置也同理。

```shell
bind 0.0.0.0
daemonize yes
port 26379
logfile "/myredis/sentinel26379.log"
pidfile "/var/run/redis-sentinel26379.pid"
dir "/myredis"
sentinel monitor mymaster 192.168.122.132 7001 2
sentinel auth-pass mymaster root
# Generated by CONFIG REWRITE
protected-mode no
user default on nopass ~* &* +@all
sentinel myid c710c5461f8d6acec712d350cd8a34a5fef34d9f
sentinel config-epoch mymaster 0
sentinel leader-epoch mymaster 0
sentinel current-epoch 0
sentinel known-replica mymaster 192.168.122.133 7002
sentinel known-replica mymaster 192.168.122.134 7003
sentinel known-sentinel mymaster 192.168.122.137 26381 5e1b76aa73f25d91b0db2a4870073393ae9484e7
sentinel known-sentinel mymaster 192.168.122.136 26380 d5511f23142adb41942b0f22dffea05415dc78c7
```

## 测试1：master7001宕机，哨兵能否故障转移重新选主

1.master7001关闭服务端

```shell
127.0.0.1:7001> shutdown
not connected> quit
```

2.查看从机是否正常

先是查询失败，提示Broken pipe客户端和服务器之间的连接被意外关闭

```shell
127.0.0.1:7002> get k1
Error: Broken pipe

```

```shell
127.0.0.1:7003> get k1
Error: Broken pipe

```

再次查询就能查出数据。

```shell
127.0.0.1:7002> get k1
"v1"
```

```shell
127.0.0.1:7003> get k1
"v1"
```

哨兵需要一定的时间来检测 master 的宕机，并进行故障转移。在这段时间内，如果客户端尝试查询，可能会因为旧的 master 已经不可达而失败。

第一次查询失败是因为客户端在尝试连接到旧的 master 地址（因为它可能还没有从 Sentinel 集群获取到最新的配置信息），第二次查询成功是因为客户端已经更新了其集群配置，并连接到了新的 master 节点。

3.是否会从剩下的2个slave中选出新的master

7002是slave，master是7003，选出了新的master

```shell
127.0.0.1:7002> info replication
# Replication
role:slave
master_host:192.168.122.134
master_port:7003
master_link_status:up
master_last_io_seconds_ago:1
master_sync_in_progress:0
slave_read_repl_offset:534653
slave_repl_offset:534653
slave_priority:100
slave_read_only:1
replica_announced:1
connected_slaves:0
master_failover_state:no-failover
master_replid:aa4199d91ec817f5e599abda2d50b57c8617c051
master_replid2:931607db8c3d02fe95ccc3ed57119300fd24139f
master_repl_offset:534653
second_repl_offset:335273
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:253
repl_backlog_histlen:534401
```



```shell
127.0.0.1:7003> info replication
# Replication
role:master
connected_slaves:1
slave0:ip=192.168.122.133,port=7002,state=online,offset=545888,lag=0
master_failover_state:no-failover
master_replid:aa4199d91ec817f5e599abda2d50b57c8617c051
master_replid2:931607db8c3d02fe95ccc3ed57119300fd24139f
master_repl_offset:545888
second_repl_offset:335273
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:281
repl_backlog_histlen:545608
```

结论：master7001宕机，哨兵自动故障转移重新选主

## 测试2：之前宕机的master7001重启后，会不会出现双master的问题？

启动7001redis服务端

```shell
[root@redis-7001 ~]# redis-server /opt/redis-6.2.6/redis7001.conf 
[root@redis-7001 ~]# redis-cli -a root -p 7001
Warning: Using a password with '-a' or '-u' option on the command line interface may not be safe.
127.0.0.1:7001> 
```

稍等一会，等待更新集群配置，7001已经作为slave从机了。

```shell
127.0.0.1:7001> info replication
Error: Server closed the connection
127.0.0.1:7001> info replication
# Replication
role:slave
master_host:192.168.122.134
master_port:7003
master_link_status:up
master_last_io_seconds_ago:1
master_sync_in_progress:0
slave_read_repl_offset:625164
slave_repl_offset:625164
slave_priority:100
slave_read_only:1
replica_announced:1
connected_slaves:0
master_failover_state:no-failover
master_replid:aa4199d91ec817f5e599abda2d50b57c8617c051
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:625164
second_repl_offset:-1
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:609384
repl_backlog_histlen:15781
```

结论：之前宕机的master7001重启后，变成了slave从机，不会出现双master问题。

## 对比前后的sentinel配置

老sentinel26379.conf

```shell
bind 0.0.0.0
daemonize yes
port 26379
logfile "/myredis/sentinel26379.log"
pidfile "/var/run/redis-sentinel26379.pid"
dir "/myredis"
sentinel monitor mymaster 192.168.122.132 7001 2
sentinel auth-pass mymaster root
# Generated by CONFIG REWRITE
protected-mode no
user default on nopass ~* &* +@all
sentinel myid c710c5461f8d6acec712d350cd8a34a5fef34d9f
sentinel config-epoch mymaster 0
sentinel leader-epoch mymaster 0
sentinel current-epoch 0
sentinel known-replica mymaster 192.168.122.133 7002
sentinel known-replica mymaster 192.168.122.134 7003
sentinel known-sentinel mymaster 192.168.122.137 26381 5e1b76aa73f25d91b0db2a4870073393ae9484e7
sentinel known-sentinel mymaster 192.168.122.136 26380 d5511f23142adb41942b0f22dffea05415dc78c7
```

新sentinel26379.conf

```shell
bind 0.0.0.0
daemonize yes
port 26379
logfile "/myredis/sentinel26379.log"
pidfile "/var/run/redis-sentinel26379.pid"
dir "/myredis"
sentinel monitor mymaster 192.168.122.134 7003 2
sentinel auth-pass mymaster root
# Generated by CONFIG REWRITE
protected-mode no
user default on nopass ~* &* +@all
sentinel myid c710c5461f8d6acec712d350cd8a34a5fef34d9f
sentinel config-epoch mymaster 1
sentinel leader-epoch mymaster 1
sentinel current-epoch 1
sentinel known-replica mymaster 192.168.122.132 7001
sentinel known-replica mymaster 192.168.122.133 7002
sentinel known-sentinel mymaster 192.168.122.137 26381 5e1b76aa73f25d91b0db2a4870073393ae9484e7
sentinel known-sentinel mymaster 192.168.122.136 26380 d5511f23142adb41942b0f22dffea05415dc78c7
```

主从配置信息发生了变化

## 对比前后redis.conf配置

redis7001.conf新增配置 新增replicaof 192.168.122.134 7003指定它的master

```shell
# Generated by CONFIG REWRITE
save 3600 1
save 300 100
save 60 10000
user default on #4813494d137e1631bba301d5acab6e7bb7aa74ce1185d456565ef51d737677b2 ~* &* +@all
replicaof 192.168.122.134 7003
```

redis7002.conf新增配置，并且replicaof 192.168.122.132 7001依然存在

```shell
# Generated by CONFIG REWRITE
save 3600 1
save 300 100
save 60 10000
user default on #4813494d137e1631bba301d5acab6e7bb7aa74ce1185d456565ef51d737677b2 ~* &* +@all
```

redis7003.conf新增配置，并且replicaof 192.168.122.132 7001已被删除

```shell
# Generated by CONFIG REWRITE
save 3600 1
save 300 100
save 60 10000
user default on #4813494d137e1631bba301d5acab6e7bb7aa74ce1185d456565ef51d737677b2 ~* &* +@all
```

结论：相关文件的内容，在运行期间会被sentinel动态进行更改。Master-Slave切换后，master的redis.conf,slave的redis.conf和sentinel.conf的内容都会发生改变,即原master的redis.conf中会多一行slaveof的配置, sentinel.conf的监控目标会随之调换





# 哨兵工作流程

## 哨兵监控

每个Redis实例都会配置一个或多个哨兵，哨兵通过发送PING命令来发现Redis实例并建立监控连接。

当主从配置中的master失效之后，sentinel会选举出一个新的master自动接替原master工作，主从配置中的其他redis服务器自动指向新的master同步数据。一般建议sentinel采取奇数台，防止某一台sentinel无法连接到master导致误切换。

## SDown主观下线(Subjectively Down)

![image-20240306191205252](F:\note\image\image-20240306191205252.png)

SDown(主观不可用)：某个sentinel节点发现某实例未在规定时间响应，则认为该实例**主观下线**。

单个sentinel节点发现某个master实例未在规定时间响应。Sentinel基于心跳机制监测服务状态，每隔1秒向集群的每个实例发送ping命令，在一定时间内都没有收到合法的回复,就达到了SDOWN的条件。

sentinel配置文件中的`down-after-milliseconds`设置了判断主观下线的时间长度

```shell
sentinel down-after-milliseconds <masterName> <timeout>
```

## ODOWN客观下线(Objectively Down)

若超过指定数量（quorum）的sentinel都认为该master实例主观下线，则该master实例**客观下线**，进行故障转移。quorum值最好超过Sentinel实例数量的一半。

```shell
sentinel monitor <master-name> <ip> <redis-port> <quorum>
```

quorum参数作为客观下线的一个依据。因为有的时候，某个sentinel节点可能因为自身网络原因导致无法连接master，而此时master并没有出现故障，所以这就需要多个sentinel都一致认为master有问题，才可以进行故障转移。

## 选举leader

当主节点被判断客观下线以后,各个哨兵节点会进行协商,先选举出一个领导者leader并由该领导者leader节点进行failover (故障迁移)

1. 每个 Sentinel 实例都会向其他 Sentinel 实例发送自己成为leader的请求。

2. Sentinel 实例会根据一定的规则（如基于配置纪元或日志条目的序号）来比较自己和其他 Sentinel 实例的优先级，选择优先级最高的 Sentinel 实例作为leader。

3. 一旦选举出leader，其他 Sentinel 实例会向其发送确认消息，以确认其领导地位。

4. leader 负责执行故障迁移操作，包括选择一个新的主 Redis 实例、更新其他 Redis 实例和 Sentinel 实例的配置等。

   

### 选举leader原理

监视该主节点的所有哨兵都有可能被选为领导者，选举使用的算法是Raft一致性算法：Raft一致性算法的基本思路是先到先得：即在一轮选举中,哨兵A向B发送成为领导者的申请,如果B没有同意过其他哨兵,则会同意A成为领导者

![image-20240306192017396](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403061920403.png)



## 选主master

一旦发现master故障，sentinel需要在salve中选择一个作为新的master，选择依据是这样的：

- 首先会判断slave节点与master节点断开时间长短，如果超过指定值（down-after-milliseconds * 10）则会排除该slave节点（剩下健康的slave节点）
- 然后判断slave节点的slave-priority值，越小优先级越高，如果是0则永不参与选举
- 如果优先级一样，则判断slave节点的offset值，越大说明数据越新，优先级越高
- 最后是判断slave节点的运行id大小，越小优先级越高。每个redis实例启动后都会随机生成一个唯一的40位的runid

![image-20240306192205238](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403061922198.png)

## 当选出一个新的master后，该如何实现切换呢？

流程如下：

- sentinel给备选的slave节点发送slaveof no one命令，让该节点成为master
- sentinel给所有其它slave发送slaveof 192.168.122.134 7003 命令，让这些slave成为新master的从节点，开始从新的master上同步数据。
- 最后修改故障节点的配置，sentinel将故障节点标记为slave，当故障节点恢复后会自动成为新的master的slave节点



以上故障迁移failover均有sentinel自动完成，无需人工干预。

# 总结

- 哨兵节点的数量应为多个,哨兵应该是集群架构,保证高可用

- 哨兵节点的数量应该是奇数（大于等于3）

- 各个哨兵节点的配置应一致

- 如果哨兵节点部署在Docker等容器里面,尤其要注意端口的正确映射

- 哨兵集群+主从复制,并不能保证数据零丢失。