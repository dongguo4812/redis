# Redis的主要性能瓶颈：网络IO

前面提到Redis是基于内存操作的，因此它的瓶颈是物理机器的内存或网络带宽而并非CPU。

![image-20240309105939960](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191543421.png)

现在服务器内存能达到16GB、32GB甚至更高。

随着网络硬件的性能提升，Redis的性能瓶颈有时会出现在网络IO的处理上，单个主线程处理网络请求的速度跟不上底层网络硬件的速度。

由于Redis的大部分操作都是在内存中完成的，因此IO操作通常成为性能瓶颈。通过采用IO多路复用（epoll技术），Redis能够更有效地处理网络IO，从而进一步提高其吞吐量。

Ngin也是使用IO多路复用（epoll技术）。

# 用户空间与内核空间

服务器大多都采用Linux系统，这里我们以Linux为例来讲解:

ubuntu和Centos 都是Linux的发行版，发行版可以看成对linux包了一层壳，任何Linux发行版，其系统内核都是Linux。我们的应用都需要通过Linux内核与硬件交互

![image-20240327153119019](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403271531124.png)

用户的应用，比如redis，mysql等其实是没有办法去执行访问我们操作系统的硬件的，所以我们可以通过发行版的这个壳子去访问内核，再通过内核去访问计算机硬件

![image-20240327153139128](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403271531375.png)



计算机硬件包括，如cpu，内存，网卡等等，内核（通过寻址空间）可以操作硬件的，但是内核需要不同设备的驱动，有了这些驱动之后，内核就可以去对计算机硬件去进行 内存管理，文件系统的管理，进程的管理等等

![image-20240327153155764](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403271531764.png)

我们想要用户的应用来访问，计算机就必须要通过对外暴露的一些接口，才能访问到，从而简介的实现对内核的操控，但是内核本身上来说也是一个应用，所以他本身也需要一些内存，cpu等设备资源，用户应用本身也在消耗这些资源，如果不加任何限制，用户去操作随意的去操作我们的资源，就有可能导致一些冲突，甚至有可能导致我们的系统出现无法运行的问题，因此我们需要把用户应用和**内核隔离开**



进程的寻址空间划分成两部分：**内核空间、用户空间**

什么是寻址空间呢？我们的应用程序也好，还是内核空间也好，都是没有办法直接去物理内存的，而是通过分配一些虚拟内存映射到物理内存中，我们的内核和应用程序去访问虚拟内存的时候，就需要一个虚拟地址，这个地址是一个无符号的整数，比如一个32位的操作系统，他的带宽就是32，他的虚拟地址就是2的32次方，也就是说他寻址的范围就是0~2的32次方， 这片寻址空间对应的就是2的32个字节，就是4GB，这个4GB，会有3个GB分给用户空间，会有1GB给内核系统

![image-20240327153355253](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403271533405.png)

在linux中，他们权限分成两个等级，0和3，用户空间只能执行受限的命令R3（Ring3），而且不能直接调用系统资源，必须通过内核提供的接口来访问内核空间可以执行特权命令R0（Ring0），调用一切系统资源，所以一般情况下，用户的操作是运行在用户空间，而内核运行的数据是在内核空间的，而有的情况下，一个应用程序需要去调用一些特权资源，去调用一些内核空间的操作，所以此时他俩需要在用户态和内核态之间进行切换。



比如：

Linux系统为了提高IO效率，会在用户空间和内核空间都加入缓冲区：

写数据时，要把用户缓冲数据拷贝到内核缓冲区，然后写入设备

读数据时，要从设备读取数据到内核缓冲区，然后拷贝到用户缓冲区

针对这个操作：我们的用户在写读数据时，会去向内核态申请，想要读取内核的数据，而内核数据要去等待驱动程序从硬件上读取数据，当从磁盘上加载到数据之后，内核会将数据写入到内核的缓冲区中，然后再将数据拷贝到用户态的buffer中，然后再返回给应用程序，整体而言，速度慢，就是这个原因，为了加速，我们希望read也好，还是wait for data也最好都不要等待，或者时间尽量的短。

![image-20240327153430301](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403271534289.png)





# Unix网络编程的五种IO模型

## **Blocking IO 阻塞IO**

在阻塞IO模型中，进程发起一个IO操作后会被挂起，直到该操作完成为止。在此期间，进程不能进行其他任何操作。这是最传统也是最简单的IO模型。

应用程序想要去读取数据，他是无法直接去读取磁盘数据的，他需要先到内核里边去等待内核操作硬件拿到数据，这个过程就是1，是需要等待的，等到内核从磁盘上把数据加载出来之后，再把这个数据写给用户的缓存区，这个过程是2，如果是阻塞IO，那么整个过程中，用户从发起读请求开始，一直到读取到数据，都是一个阻塞状态。

![image-20240327153603100](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403271536073.png)

具体流程如下图：

用户去读取数据时，会去先发起recvform一个命令，去尝试从内核上加载数据，如果内核没有数据，那么用户就会等待，此时内核会去从硬件上读取数据，内核读取数据之后，会把数据拷贝到用户态，并且返回ok，整个过程，都是阻塞等待的，这就是阻塞IO

总结如下：

顾名思义，阻塞IO就是两个阶段都必须阻塞等待：

**阶段一：**

- 用户进程尝试读取数据（比如网卡数据）
- 此时数据尚未到达，内核需要等待数据
- 此时用户进程也处于阻塞状态

阶段二：

- 数据到达并拷贝到内核缓冲区，代表已就绪
- 将内核数据拷贝到用户缓冲区
- 拷贝过程中，用户进程依然阻塞等待
- 拷贝完成，用户进程解除阻塞，处理数据

可以看到，阻塞IO模型中，用户进程在两个阶段都是阻塞状态。

![image-20240327153644437](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403271536376.png)

 当用户进程调用了recvfrom这个系统调用, kernel就开始了IO的第一个阶段:准备数据(对于网络IO来说,很多时候数据在一开始还没有到达。比如,还没有收到一个完整的UDP包。这个时候kernel就要等待足够的数据到来) 。这个过程需要等待,也就是说数据被拷贝到操作系统内核的缓冲区中是需要一个过程的。而在用户进程这边,整个进程会被阻塞(当然,是进程自己选择的阻塞) 。当kernel一直等到数据准备好了,它就会将数据从kernel中拷贝到用户内存,然后kernel返回结果,用户进程才解除block的状态,重新运行起来。所以,BIO的特点就是在IO执行的两个阶段都被block了。

## **NoneBlocking IO  非阻塞IO**

非阻塞IO模型允许进程发起一个IO操作后不被阻塞，即进程可以继续执行其他操作。不过，进程需要不时地检查IO操作是否完成，通常通过轮询的方式进行。

顾名思义，非阻塞IO的recvfrom操作会立即返回结果而不是阻塞用户进程。

阶段一：

- 用户进程尝试读取数据（比如网卡数据）
- 此时数据尚未到达，内核需要等待数据
- 返回异常给用户进程
- 用户进程拿到error后，再次尝试读取
- 循环往复，直到数据就绪

阶段二：

- 将内核数据拷贝到用户缓冲区
- 拷贝过程中，用户进程依然阻塞等待
- 拷贝完成，用户进程解除阻塞，处理数据





![image-20240327153759048](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403271538154.png)

可以看到，非阻塞IO模型中，用户进程在第一个阶段是非阻塞，第二个阶段是阻塞状态。虽然是非阻塞，但性能并没有得到提高。而且忙等机制会导致CPU空转，CPU使用率暴增。



## **IO multiplexing IO多路复用**

l/O多路复用，经典的Reactor设计模式（基于事件驱动的设计模式），简单来说就是通过监听文件的读写事件再通知线程执行相关操作。



## **signal driven IO 信号驱动IO**

信号驱动IO模型下，当某个IO操作准备就绪时，进程会收到一个信号。随后，进程可以通过系统调用来处理这个IO操作。

信号驱动IO是与内核建立SIGIO的信号关联并设置回调，当内核有FD就绪时，会发出SIGIO信号通知用户，期间用户应用可以执行其它业务，无需阻塞等待。

阶段一：

- 用户进程调用sigaction，注册信号处理函数
- 内核返回成功，开始监听FD
- 用户进程不阻塞等待，可以执行其它业务
- 当内核数据就绪后，回调用户进程的SIGIO处理函数

阶段二：

- 收到SIGIO回调信号
- 调用recvfrom，读取
- 内核将数据拷贝到用户空间
- 用户进程处理数据

![image-20240327154146762](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403271541537.png)

当有大量IO操作时，信号较多，SIGIO处理函数不能及时处理可能导致信号队列溢出，而且内核空间与用户空间的频繁信号交互性能也较低。

## **asynchronous IO 异步IO**

异步IO模型是这五种模型中最复杂的一种。在此模型中，进程发起一个IO操作后便开始其他任务。当IO操作完成时，内核会自动将结果通知给进程，而无需进程主动查询或者等待信号。

这种方式，不仅仅是用户态在试图读取数据后，不阻塞，而且当内核的数据准备完成后，也不会阻塞

他会由内核将所有数据处理完成后，由内核将数据写入到用户态中，然后才算完成，所以性能极高，不会有任何阻塞，全部都由内核完成，可以看到，异步IO模型中，用户进程在两个阶段都是非阻塞状态。

![image-20240327154211076](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403271542115.png)

整个过程都是非阻塞的，用户进程调用完异步api后就可以做其他事情，内核等待数据就绪并拷贝到用户空间后才会递交信号，通知用户进程

高并发下，内核可能出现崩溃

## FileDescriptor文件描述符

FileDescriptor：文件描述符，简称FD，句柄

文件描述符（File descriptor)是计算机科学中的一个术语，是一个用于表述指向文件的引用的抽象化概念。文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，文件描述符这一概念往往只适用于UNIX、Linux这样的操作系统。

![image-20240309140234916](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191543128.png)

# 同步、异步、阻塞、非阻塞

1. **同步（Synchronous）与异步（Asynchronous）**：
   - **同步**：指一个任务的完成情况可以直接得到。在同步通信中，发送端必须等待接收端返回结果后，才能继续执行后续任务。在等待过程中，发送端不能执行其他任务。这就像小明去烧水，他站在炉子旁边等待，直到水烧开后才去做其他事情。
   - **异步**：指一个任务的完成情况不可以直接得到，需要后续等待操作系统通知。在异步通信中，发送端发出请求后，不必等待接收端的响应，可以立即执行其他任务。当接收端处理完请求后，会通知发送端。这就像一个快递员去送包裹，他不必等待收件人签收，就可以继续送下一个包裹，等收件人签收后再通知他。
2. **阻塞（Blocking）与非阻塞（Non-blocking）**：
   - **阻塞**：指调用结果返回之前，当前线程会被挂起，一直处于等待消息通知的状态，不能执行其他操作。这就像小明去超市排队结账，如果前面有很多人，他必须等待，直到轮到他为止，这期间他不能做其他事情。
   - **非阻塞**：指调用结果返回之前，该函数不会阻塞当前线程，会立刻返回，但想要得到这些结果需要不停地进行询问。这就像小明去超市自助结账，即使前面有人在结账，他也可以先去拿东西，然后时不时回来看看是否轮到自己。

结合以上概念，我们可以得出以下四种通信模式：

- **同步阻塞**：发送端发送请求后必须等待接收端处理并返回结果，期间不能执行其他任务。接收端在处理请求时也会阻塞，不能处理其他请求。这就像小明去银行办理业务，必须等待柜员处理完当前业务后才能办理，而柜员也只能处理一个业务。
- **同步非阻塞**：发送端发送请求后等待接收端处理并返回结果，但期间可以执行其他任务。接收端在处理一个请求时不会阻塞，可以处理其他请求。这就像小明在餐厅点餐，他点完餐后可以去拿饮料或者看菜单，而服务员也可以同时处理其他客人的点餐。
- **异步阻塞**：发送端发送请求后不等待接收端处理，可以继续执行其他任务。但接收端在处理请求时会阻塞，直到处理完才能处理其他请求。这种模式在实际应用中较少见，因为它没有充分利用异步的优势。
- **异步非阻塞**：发送端发送请求后不等待接收端处理，可以继续执行其他任务。接收端在处理一个请求时也不会阻塞，可以处理其他请求。这就像小明在网上购物，他下单后可以继续浏览其他商品或做其他事情，而电商平台也可以同时处理其他用户的订单。

# IO多路复用

IO多路复用是一种同步的IO模型，实现一个线程监测多个文件句柄，一个某个文件句柄就绪就能够通知到对应应用程序进行相应的读写操作，没有文件句柄就绪时就会阻塞应用程序，从而释放CPU资源。

即使用单个线程监听多个连接请求。

IO：网络IO，在操作系统层面指数据在内核态和用户态之间的读写操作。

多路：指的是多个客户端连接，即多个socket。

复用指的是线程的复用，即一个线程可以同时处理多个客户端连接。

多路复用主要有三种技术：select、poll、epoll。

epoll是最新的也是目前最好的多路复用技术。采用多路l/O复用技术可以让单个线程高效的处理多个连接请求（尽量减少网络IO的时间消耗）。



## 多路复用解决的问题

并发多客户端连接，在多路复用之前最常用的方案是 同步阻塞网络IO模型

这种模式就是用一个进程来处理一个网络连接，每个用户请求都要分配一个进程来处理。

如果需要一个进程处理多个客户端连接。那么就可以使用IO多路复用对进程实现复用。

## Redis单线程如何处理那么多并发客户端连接的

Redis利用epoll来实现IO多路复用，将连接信息和事件放到队列中，一次放到文件事件分派器，事件分派器将事件分发给事件处理器。

![image-20240319100153089](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191543918.png)

Redis是单线程的,所有的操作都是按照顺序线性执行的,但是由于读写操作等待用户输入或输出都是阻塞的,所以IO操作在一般情况下往往不能直接返回,这会导致某一文件的IO阻塞导致整个进程无法对其它客户提供服务,而IO多路复用就是为了解决这个问题而出现

所谓1IO多路复用机制,就是说通过一种机制,可以监视多个描述符,一旦某个描述符就绪(一般是读就绪或写就绪),能够通知程序进行相应的读写操作。这种机制的使用需要select、poll、epoll来配合。多个连接共用一个阻塞对象,应用程序只需要在一个阻塞对象上等待,无需阻塞等待所有连接。当某条连接有新的数据可以处理时，操作系统通知应用程序，线程从阻塞状态返回，开始进行业务处理。



Redis服务采用Reactor的方式来实现文件事件处理器(每一个网络连接其实都对应一个文件描述符)

Redis基于Reactor模式开发了网络事件处理器,这个处理器被称为文件事件处理器。它的组成结构为4部分:

- 多个套接字、

- IO多路复用程序、

- 文件事件分派器、

- 事件处理器。



因为文件事件分派器队列的消费是单线程的,所以Redis才叫单线程模型



![image-20240319100845694](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191543996.png)

# Redis实战

需要准备1个redis server， 2个redis client

## BIO

当用户进程调用了recvfrom这个系统调用，kernel（操作系统的核心组件，即操作系统内核）就开始了IO的第一个阶段:准备数据(对于网络IO来说,很多时候数据在一开始还没有到达。比如,还没有收到一个完整的UDP包。这个时候kernel就要等待足够的数据到来)。

这个过程需要等待,也就是说数据被拷贝到操作系统内核的缓冲区中是需要一个过程的。而在用户进程这边,整个进程会被阻塞(当然,是进程自己选择的阻塞) 。当kernel一直等到数据准备好了,它就会将数据从kernel中拷贝到用户内存,然后kernel返回结果,用户进程才解除block的状态,重新运行起来。所以, BIO的特点就是在IO执行l的两个阶段都被block了。

![image-20240319103125236](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191543245.png)

recvfrom()：从(已连接)套接口上接收数据,并捕获数据发送源的地址。

### accept

`accept`调用会一直阻塞当前线程，直到有一个新的连接到来。

RedisServer服务端

```java
import java.io.IOException;
import java.net.ServerSocket;


/**
 * 服务端
 */
public class RedisServer {
    public static void main(String[] args) throws IOException {
        ServerSocket serverSocket = new ServerSocket(6379);

        while (true){
            System.out.println("1 模拟RedisServer启动，等待客户端连接中");
            serverSocket.accept();
            System.out.println("2 客户端连接成功");
            System.out.println("=============================");
        }
    }
}
```

RedisClient1客户端1

```java
import java.io.IOException;
import java.net.Socket;

/**
 * 客户端1
 */
public class RedisClient1 {
    public static void main(String[] args) throws IOException {
        System.out.println("RedisClient1 请求连接");
        Socket socket = new Socket("127.0.0.1", 6379);
        System.out.println("RedisClient1 连接结束");
    }
}
```

RedisClient2客户端2

```java
import java.io.IOException;
import java.net.Socket;

/**
 * 客户端2
 */
public class RedisClient2 {
    public static void main(String[] args) throws IOException {
        System.out.println("RedisClient2 请求连接");
        Socket socket = new Socket("127.0.0.1", 6379);
        System.out.println("RedisClient2 连接结束");
    }
}
```



1.先启动RedisServer，阻塞等待客户端连接

![image-20240319110611572](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191543702.png)

2.启动RedisClient1，连接成功

![image-20240319110727500](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191543632.png)

3.RedisServer处理RedisClient1连接请求后，再次进入阻塞，等待客户端的连接

![image-20240319110710844](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191543696.png)

4.启动RedisClient2，连接成功

![image-20240319110842752](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191543305.png)

5.RedisServer处理RedisClient2连接请求后，再次进入阻塞，等待客户端的连接

![image-20240319110904457](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191543478.png)

### read

1启动RedisServer

![image-20240319113929322](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191543800.png)

2.启动RedisClient1，等待输入

![image-20240319113953884](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191543209.png)

3.RedisServer处理RedisClient1连接请求后，进入阻塞等待读取

![image-20240319114026968](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191543976.png)

4.RedisClient1输入 客户端1第一次请求

![image-20240319131156873](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191542151.png)

5.RedisServer读取我是客户端1

![image-20240319131207949](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191542502.png)

6.启动RedisClient2，等待输入

![image-20240319131219405](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191542633.png)

7.RedisClient2输入 客户端2第一次请求

![image-20240319131239580](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191542701.png)

8.RedisServer并未读取到客户端2第一次请求

![image-20240319131303780](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191542167.png)

9.RedisClient2输入 客户端2第二次请求

![image-20240319131326440](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191542258.png)

10.RedisServer还是并未读取到客户端2第二次请求

![image-20240319131348236](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191542363.png)

11.RedisClient1输入 quit 断开连接

![image-20240319131418916](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191542319.png)

12.RedisServer在RedisClient1断开连接后，连接RedisClient2，接收到RedisClient2之前所有的输入

![image-20240319131500088](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191542558.png)





可以看出BIO存在很大的问题，当客户端与服务端建立了连接，如果这个连接的客户端不发送数据，服务端就会一直堵塞在read方法，其他客户端就不能进行连接。服务端一次只能处理一个客户端连接。

那该如何解决这个问题呢？

### 利用多线程模型

只要连接了一个socket，就分配一个线程来处理，read方法阻塞在每个线程上，这样就能操作多个socket了。

程序服务端只负责监听是否有客户端连接，使用 accept（）阻塞。

客户端1连接服务端,就开辟一个线程(thread1)来执行read()方法,程序服务端继续监听

客户端2连接服务端,也开辟一个线程(thread2)来执行read()方法,程序服务端继续监听

客户端3连接服务端,也开辟一个线程(thread3)来执行read()方法,程序服务端继续监听

.........



任何一个线程上的socket有数据发送过来，就能立马从read方法中读到。

RedisServer

```java
import java.io.IOException;
import java.io.InputStream;
import java.net.ServerSocket;
import java.net.Socket;

/**
 * 服务端
 */
public class RedisServer {
    public static void main(String[] args) throws IOException {
        ServerSocket serverSocket = new ServerSocket(6379);
        while (true) {
            System.out.println("1 等待客户端连接中");
            Socket socket = serverSocket.accept();
            System.out.println("2 客户端连接成功");
            new Thread(() -> {
                try {
                    InputStream inputStream = socket.getInputStream();
                    int length = -1;
                    byte[] bytes = new byte[1024];
                    System.out.println("3  等待读取");
                    while ((length = inputStream.read(bytes)) != -1) {
                        System.out.println("读取成功" + new String(bytes, 0, length));
                        System.out.println("=========================================");
                    }
                    inputStream.close();
                    socket.close();
                } catch (IOException e) {
                    throw new RuntimeException(e);
                }
            }, Thread.currentThread().getName()).start();
        }
    }
}

```



RedisClient和read中一致。

1.启动RedisServer

![image-20240319141241381](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191542894.png)

2.启动RedisClient1

![image-20240319141302798](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191542654.png)



3.RedisServer连接到RedisClient1，等待RedisClient1输入

![image-20240319141340135](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191542125.png)

4.RedisClient1输入 客户端1第一次请求

![image-20240319141424299](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191542750.png)



5.RedisServer接收读取

![image-20240319141439468](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191542488.png)

6.启动RedisClient12

![image-20240319141503822](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191542253.png)



7.RedisServer连接到RedisClient2，等待RedisClient2输入

![image-20240319141529342](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191542299.png)

8.RedisClient2输入 客户端2第一次请求

![image-20240319141559441](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191542727.png)

9.RedisServer读取

![image-20240319141618211](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191542825.png)

使用多线程，每来一个客户端，就要创建一个线程，如果来1万个客户端，那就要开辟1万个线程。

还会涉及到上下文的切换，十分耗费资源。

那又该如何解决呢？



办法1：使用线程池

在客户端连接少的情况下可以使用,但是如果在用户量大的情况下,是不知道线程池要多的,太大了内存可能不够,



办法2：NIO 非阻塞式IO

因为BIO中read方法是阻塞的，所以要创建多个线程，那么如果read方法不是阻塞的，就不用创建多个线程了。



tomcat7之前就是用BIO多线程来解决多连接



## NIO

当用户进程发出read操作时,如果kernel中的数据还没有准备好,那么它并不会block用户进程,而是立刻返回一个error。

从用户进程角度讲,它发起一个read操作后,并不需要等待,而是马上就得到了一个结果。用户进程判断结果是一个error时,它就知道数据还没有准备好,于是它可以再次发送read操作。一旦kernel中的数据准备好了,并且又再次收到了用户进程的system call,那么它马上就将数据拷贝到了用户内存，然后返回。

所以，NIO特点是用户进程需要不断的主动询问内核数据准备好了吗？一句话，用轮询替代阻塞！

![image-20240319142718472](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191542410.png)



在NIO模式中，一切都是非阻塞的：

accept()方法是非阻塞的,如果没有客户端连接,就返回无连接标识

read()方法是非阻塞的,如果read()方法读取不到数据就返回空闲中标识,如果读取到数据时只阻塞read()方法读数据的时间

在NIO模式中，只有一个线程：

当一个客户端与服务端进行连接，这个socket就会加入到一个数组中，隔一段时间遍历一次，看这个socket的read()方法能否读到数据，这样一个线程就能处理多个客户端的连接和读取了



1.RedisServer启动

![image-20240319144939223](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191541775.png)



2.RedisClient1启动

![image-20240319145001588](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191541897.png)

3.RedisServer  RedisClient1连接成功

![image-20240319145014129](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191541948.png)

4.RedisClient1输入 RedisClient1第一次请求



![image-20240319145053876](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191541055.png)

5.RedisServer读取到 RedisClient1第一次请求



![image-20240319145115244](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191541281.png)



6.启动RedisClient2

![image-20240319145147108](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191541418.png)

7.RedisServer RedisClient2连接成功

![image-20240319145231824](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191541630.png)

8. RedisClient2 输入 RedisClient2第一次请求

![image-20240319145302897](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191541421.png)

9.RedisServer读取到 RedisClient2第一次请求

![image-20240319145339744](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191541145.png)

NIO成功的解决了BIO需要开启多线程的问题，NIO中一个线程就能解决多个socket，但是还存在2个问题。

问题一：这个模型在客户端少的时候十分好用，但是客户端如果很多，比如有1万个客户端进行连接，那么每次循环就要遍历1万个socket，如果一万个socket中只有10个socket有数据，也会遍历一万个socket，就会做很多无用功，每次遍历遇到 read 返回 -1 时仍然是一次浪费资源的系统调用。



问题二：而且这个遍历过程是在用户态进行的,用户态判断socket是否有数据还是调用内核的read()方法实现的,这就涉及到用户态和内核态的切换,每遍历一个就要切换一次，开销依然很大。





优点:不会阻塞在内核的等待数据过程,每次发起的IO请求可以立即返回,不用阻塞等待,实时性较好。

缺点:轮询将会不断地询问内核,这将占用大量的CPU时间,系统资源利用率较低,所以一般Web服务器不使用这种IO模型。

结论:让Linux内核搞定上述需求,我们将一批文件描述符通过一次系统调用传给内核由内核层去遍历,才能真正解决这个问题。IO多路复用应运而生,也即将上述工作直接放进Linux内核,不再两态转换而是直接从内核获得结果,因为内核是非阻塞的。





## IO多路复用

I/O多路复用在英文中其实叫 I/O multiplexing

![image-20240319150535558](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191541858.png)



1/O multiplexing这里面的multiplexing指的其实是在单个线程通过记录跟踪每一个Sock(I/O流)的状态来同时管理多个IO流.目的是尽量多的提高服务器的吞吐能力。

![image-20240319150613087](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191541039.png)

大家都用过nginx， nginx使用epoll接收请求，ngnix会有很多链接进来， epoll会把他们都监视起来，然后像拨开关一样，谁有数据就拨向谁，然后调用相应的代码处理。redis类似同理



无论是阻塞IO还是非阻塞IO，用户应用在一阶段都需要调用recvfrom来获取数据，差别在于无数据时的处理方案：

如果调用recvfrom时，恰好没有数据，阻塞IO会使CPU阻塞，非阻塞IO使CPU空转，都不能充分发挥CPU的作用。如果调用recvfrom时，恰好有数据，则用户进程可以直接进入第二阶段，读取并处理数据

所以怎么看起来以上两种方式性能都不好

而在单线程情况下，只能依次处理IO事件，如果正在处理的IO事件恰好未就绪（数据不可读或不可写），线程就会被阻塞，所有IO事件都必须等待，性能自然会很差。

就比如服务员给顾客点餐，**分两步**：

- 顾客思考要吃什么（等待数据就绪）
- 顾客想好了，开始点餐（读取数据）

那么问题来了：用户进程如何知道内核中数据是否就绪呢？

所以接下来就需要详细的来解决多路复用模型是如何知道到底怎么知道内核数据是否就绪的问题了

这个问题的解决依赖于提出的

**文件描述符**（File Descriptor）：简称FD，是一个从0 开始的无符号整数，用来关联Linux中的一个文件。在Linux中，一切皆文件，例如常规文件、视频、硬件设备等，当然也包括网络套接字（Socket）。

**IO多路复用**:通过FD，我们的网络模型可以利用一个线程监听多个FD，并在某个FD可读、可写时得到通知，从而避免无效的等待，充分利用CPU资源。

阶段一：

- 用户进程调用select，指定要监听的FD集合
- 核监听FD对应的多个socket
- 任意一个或多个socket数据就绪则返回readable
- 此过程中用户进程阻塞

阶段二：

- 用户进程找到就绪的socket
- 依次调用recvfrom读取数据
- 内核将数据拷贝到用户空间
- 用户进程处理数据

当用户去读取数据的时候，不再去直接调用recvfrom了，而是调用select的函数，select函数会将需要监听的数据交给内核，由内核去检查这些数据是否就绪了，如果说这个数据就绪了，就会通知应用程序数据就绪，然后来读取数据，再从内核中把数据拷贝给用户态，完成数据处理，如果N多个FD一个都没处理完，此时就进行等待。

用IO复用模式，可以确保去读数据的时候，数据是一定存在的，他的效率比原来的阻塞IO和非阻塞IO性能都要高



![image-20240327154012918](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403271540906.png)

IO多路复用是利用单个线程来同时监听多个FD，并在某个FD可读、可写时得到通知，从而避免无效的等待，充分利用CPU资源。不过监听FD的方式、通知的方式又有多种实现，常见的有：

- select
- poll
- epoll

其中select和pool相当于是当被监听的数据准备好之后，他会把你监听的FD整个数据都发给你，你需要到整个FD中去找，哪些是处理好了的，需要通过遍历的方式，所以性能也并不是那么好

而epoll，则相当于内核准备好了之后，他会把准备好的数据，直接发给你，咱们就省去了遍历的动作。









IO multiplexing就是我们说的select, poll, epoll,有些技术书籍也称这种IO方式为event driven IO事件驱动IO。就是通过一种机制,一个进程可以监视多个描述符,一旦某个描述符就绪(一般是读就绪或者写就绪) ,能够通知程序进行相应的读写操作。可以基于一个阻塞对象并同时在多个描述符上等待就绪,而不是使用多个线程(每个文件描述符一个线程,每次new一个线程),这样可以大大节省系统资源。

所以,IO多路复用的特点是通过一种机制一个进程能同时等待多个文件描述符而这些文件描述符(套接字描述符)其中的任意一个进入读就绪状态1select, poll,epoll等函数就可以返回。

![image-20240319150711339](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191541804.png)



将用户socket对应的fd注册进epoll，然后epoll帮你监听哪些socket上有消息到达，这样就避免了大量的无用操作。此时的socket应该采用非阻塞模式。这样,整个过程只在调用select, poll,epoll这些调用的时候才会阻塞,收发客户消息是不会阻塞的,整个进程或者线程就被充分利用起来,这就是事件驱动,所谓的reactor反应模式。





Reactor模式,是指通过一个或多个输入同时传递给服务处理器的服务请求的事件驱动处理模式。服务端程序处理传入多路请求,并将它们同步分派给请求对应的处理线程, Reactor模式也叫Dispatcher模式。即IO多了复用统一监听事件,收到事件后分发(Dispatch给某进程),是编写高性能网络服务器的必备技术。

![image-20240319151449165](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191541757.png)



Reactor 模式中有 2 个关键组成：

1.Reactor: Reactor在一个单独的线程中运行,负责监听和分发事件,分发给适当的处理程序来对IO事件做出反应。它就像公司的电话接线员，它接听来自客户的电话并将线路转移到适当的联系人；

 2.Handlers：处理程序执行 IO事件要完成的实际事件，类似于客户想要与之交谈的公司中的实际办理人。Reactor 通过调度适当的处理程序来响应IO事件，处理程序执行非阻塞操作。

### select

select是linux中最早的IO多路复用实现方案

![image-20240327155231787](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403271552995.png)

![image-20240319151741264](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191541673.png)

select 函数监视的文件描述符分3类，分别是readfds、writefds和exceptfds，将用户传入的数组拷贝到内核空间

调用后select函数会阻塞,直到有描述符就绪(有数据可读、可写、或者有except)或超时(timeout指定等待时间,如果立即返回设为null即可），函数返回。

当select函数返回后，可以通过遍历fdset，来找到就绪的描述符。

![image-20240327155855210](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403271558131.png)



C语言实现代码

![image-20240319152208156](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191541454.png)

![image-20240319152138528](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191541655.png)

![image-20240319152314067](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191541059.png)

select其实就是把NIO中用户态要遍历的fd数组(我们的每一个socket链接,安装进ArrayList里面的那个)拷贝到了内核态,让内核态来遍历,因为l用户态判断socket是否有数据还是要调用内核态的，所有拷贝到内核态后，这样遍历判断的时候就不用一直用户态和内核态频繁切换了

从代码中可以看出，select系统调用后，返回了一个置位后的&rset，这样用户态只需进行很简单的二进制比较，就能很快知道哪些socket需要read数据，有效提高了效率



![image-20240319152414622](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191541488.png)









1、bitmap最大1024位,一个进程最多只能处理1024个客户端

2、&rset不可重用,每次socket有数据就相应的位会被置位

3、文件描述符数组拷贝到了内核态(只不过无系统调用切换上下文的开销。(内核层可优化为异步事件通知) ),仍然有开销。select调用需要传入fd数组,需要拷贝一份到内核,高并发场景下这样的拷贝消耗的资源是惊人的。(可优化为不复制)

4、 select并没有通知用户态哪一个socket有数据,仍然需要O(n)的遍历。select仅仅返回可读文件描述符的个数，具体哪个可读还是要用户自己遍历。(可优化为只返回给用户就绪的文件描述符,无需用户做无效的遍历)

### poll

poll模式对select模式做了简单改进，但性能提升不明显，部分关键代码如下：

![image-20240327160006190](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403271600194.png)

![image-20240319152613924](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191540392.png)

![image-20240319152704916](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191540666.png)

![image-20240319152645991](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191540219.png)

解决的问题

1、 poll使用pollfd数组来代替select中的bitmap,数组没有1024的限制,可以一次管理更多的client。它和select的主要区别就是,去掉了select 只能监听 1024 个文件描述符的限制。

2、当pollfds数组中有事件发生,相应的revents置位为1,遍历的时候又置位回零,实现了pollfd数组的重用

poll其本质原理还是select的方法，还存在select中原来的问题

1、pollfds数组拷贝到了内核态，仍然有开销

2、poll并没有通知用户态哪一个socket有数据，仍然需要O(n)的遍历

### epoll

epoll模式是对select和poll的改进，它提供了三个函数：



![image-20240327160414879](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403271604462.png)





![image-20240319152936394](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191540490.png)

#### epoll_create

创建一个epoll句柄

#### epoll_ctl

向内核添加 修改或删除要监控的文件描述符

#### epoll_wait

类似发起select()调用





![image-20240319153152232](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191540394.png)





多路复用快的原因在于,操作系统提供了这样的系统调用,使得原来的while循环里多次系统调用,变成了一次系统调用 +内核层遍历这些文件描述符。

epoll是现在最先进的IO多路复用器, Redis、 Nginx, linux中的Java NIO都使用的是epoll.

这里“多路”指的是多个网络连接, “复用”指的是复用同一个线程。

1、一个socket的生命周期中只有一次从用户态拷贝到内核态的过程,开销小

2、使用event事件通知机制,每次socket中有数据会主动通知内核,并加入到就绪链表中,不需要遍历所有的socket



在多路复用IO模型中,会有一个内核线程不断地去轮询多个socket的状态,只有当真正读写事件发送时,才真正调用实际的1O读写操作。因为在多路复用IO模型中,只需要使用一个线程就可以管理多个socket,系统不需要建立新的进程或者线程,也不必维护这些线程和进程,并且只有真正有读写事件进行时,才会使用1O资源,所以它大大减少来资源占用。多路IO复用模型是利用select, poll、 epoll可以同时监察多个流的IO事件的能力,在空闲的时候,会把当前线程阻塞掉,当有一个或多个流有IO事件时,就从阻塞态中唤醒,于是程序就会轮询一遍所有的流(epoll是只轮询那些真正发出了事件的流),并且只依次顺序的处理就绪的流,这种做法就避免了大量的无用操作。采用多路1/。复用技术可以让单个线程高效的处理多个连接请求（尽量减少网络IO的时间消耗) ,且Redis在内存中操作数据的速度非常快,也就是说内存内的操作不会成为影响Redis性能的瓶颈

#### 事件通知机制

当FD有数据可读时，我们调用epoll_wait（或者select、poll）可以得到通知。但是事件通知的模式有两种：

- LevelTriggered：简称LT，也叫做水平触发。只要某个FD中有数据可读，每次调用epoll_wait都会得到通知。（默认模式）
- EdgeTriggered：简称ET，也叫做边沿触发。只有在某个FD有状态变化时，调用epoll_wait才会被通知。

举个栗子：

- 假设一个客户端socket对应的FD已经注册到了epoll实例中
- 客户端socket发送了2kb的数据
- 服务端调用epoll_wait，得到通知说FD就绪
- 服务端从FD读取了1kb数据回到步骤3（再次调用epoll_wait，形成循环）

如果我们采用LT模式，因为FD中仍有1kb数据，则第⑤步依然会返回结果，并且得到通知如果我们采用ET模式，因为第③步已经消费了FD可读事件，第⑤步FD状态没有变化，因此epoll_wait不会返回，数据无法读取，客户端响应超时。

![image-20240327160745082](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403271607235.png)

ET模式避免了LT模式可能出现的惊群现象

ET模式最好结合非阻塞IO读取FD数据，相比LT会复杂一些





![image-20240319153352545](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403191540236.png)

## 基于epoll模式的web服务流程

服务器启动以后，服务端会去调用epoll_create，创建一个epoll实例，epoll实例中包含两个数据

1、红黑树（为空）：rb_root 用来去记录需要被监听的FD

2、链表（为空）：list_head，用来存放已经就绪的FD

创建好了之后，会去调用epoll_ctl函数，此函数会会将需要监听的数据添加到rb_root中去，并且对当前这些存在于红黑树的节点设置回调函数，当这些被监听的数据一旦准备完成，就会被调用，而调用的结果就是将红黑树的fd添加到list_head中去(但是此时并没有完成)

3、当第二步完成后，就会调用epoll_wait函数，这个函数会去校验是否有数据准备完毕（因为数据一旦准备就绪，就会被回调函数添加到list_head中），在等待了一段时间后(可以进行配置)，如果等够了超时时间，则返回没有数据，如果有，则进一步判断当前是什么事件，如果是建立连接时间，则调用accept() 接受客户端socket，拿到建立连接的socket，然后建立起来连接，如果是其他事件，则把数据进行写出

![image-20240327160938703](https://gitee.com/dongguo4812_admin/image/raw/master/image/202403271609787.png)
